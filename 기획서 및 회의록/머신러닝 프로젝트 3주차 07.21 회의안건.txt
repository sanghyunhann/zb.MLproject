내일 회의  오후 8시

담당 모델:

김승민 : knn
박영주 : xgboost
한상현 : 로지스틱 레그레이션
한정연 : 랜덤포레스트

1. 담당 모델의 하이퍼파라미터 특성 조사와 어떻게 설정하면 좋을지 생각해오기

2. 오버샘플링기법 중 어떤 것이 적합할지 조사 및 생각해오기

-----------------------------------------------------------------------------------------------------------------
아래는 gpt에게 물어본 오버샘플링기법에 대한 답변입니다.

오버샘플링은 불균형한 클래스 분포를 가진 데이터에서 소수 클래스의 샘플을 증가시키는 기법입니다. 오버샘플링 기법은 다양한 방법으로 구현될 수 있으며 목표는 소수 클래스의 샘플을 늘려서 모델이 불균형한 데이터를 더욱 잘 학습할 수 있도록 돕는 것입니다. 몇 가지 오버샘플링 기법의 예시는 다음과 같습니다:

1. Random Oversampling: 소수 클래스의 샘플을 중복해서 늘리는 방법입니다. 이는 소수 클래스의 데이터를 반복해서 추가하여 클래스 간의 균형을 맞추는 방법입니다.

2. SMOTE (Synthetic Minority Over-sampling Technique): 소수 클래스의 샘플을 이용하여 합성된 샘플을 생성하는 방법입니다. SMOTE는 소수 클래스의 인접한 이웃을 선택하고 그 사이에 새로운 합성된 샘플을 생성하여 데이터를 증가시킵니다.

3. ADASYN (Adaptive Synthetic Sampling): ADASYN은 SMOTE의 변형으로, 소수 클래스 샘플에 가중치를 부여하여 합성된 샘플을 생성하는 방법입니다. 이는 소수 클래스에 더 많은 가중치를 주어 소수 클래스의 부족한 표현을 보완합니다.

4. SMOTENC: SMOTENC는 범주형 변수를 고려한 SMOTE의 확장된 버전입니다. 이는 범주형 변수와 연속형 변수를 모두 고려하여 합성된 샘플을 생성합니다.

5. Borderline-SMOTE: Borderline-SMOTE는 소수 클래스의 경계에 위치한 샘플을 주로 증가시키는 방법입니다. 경계에 있는 샘플들이 모델의 결정 경계를 더 잘 학습할 수 있도록 도움을 줍니다.

6. SVMSMOTE: SVMSMOTE는 Support Vector Machine (SVM) 분류기를 사용하여 소수 클래스의 샘플을 증가시키는 방법입니다. 이는 SMOTE 기법을 SVM 분류 결과에 따라 가중치를 부여하여 합성된 샘플을 생성합니다.

오버샘플링은 클래스 불균형 문제를 해결하는 방법 중 하나이며, 특정 데이터셋에 적합한 오버샘플링 기법을 선택하는 것은 문제와 데이터에 따라 다를 수 있습니다. 이러한 기법들을 활용하여 소수 클래스의 샘플을 증가시키고 데이터를 균형있게 학습시킬 수 있습니다.
 
