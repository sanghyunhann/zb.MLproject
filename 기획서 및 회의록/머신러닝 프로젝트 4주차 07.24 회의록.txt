각자의 맡은 모델의 베스트 파라미터를 찾고, 모델 평가지표 가져오기

xgboost가 정확도가 가장 높게 나옴

from xgboost import XGBClassifier

# 최적의 하이퍼파라미터로 XGBoost 모델 생성
model = XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    seed=0,
    gamma=0,
    learning_rate=0.1,
    max_depth=3,
    min_child_weight=1,
    n_estimators=300,
    subsample=0.7
)

model.fit(X_train, y_train)

--------------------------------------------------------------------------------------------------------------
오버샘플링 기법 선정

김승민 : Random Over-sampling
: 무작위로 소수 클래스에서 샘플을 복사하여 균형을 이루게 하는 방식입니다. 학습 데이터에 과적합(overfitting)될 수 있습니다.

한상현 : SMOTE (Synthetic Minority Over-sampling Technique)
: 소수 클래스의 샘플을 사용하여 가상 샘플을 생성하는 방법입니다. 이 방법에서는 소수 클래스 샘플 근처의 특성 공간에서 새로운 샘플을합니다. 그 결과, 무작위 중복 샘플링보다 더 많은 다양성을 얻을 수 있습니다.

한정연 : Borderline-SMOTE
: SMOTE와 유사하지만, 다수 클래스와 소수 클래스의 경계 근처에 있는 샘플에 초점을 맞춘 가상 표본을 생성하는 방법입니다. 이를 통해 경계 지역의 이해를 개선하고 분류기의 성능을 향상시킬 수 있습니다.

박영주 : ADASYN (Adaptive Synthetic Sampling): 소수 클래스 주변의 데이터가 다수 클래스와 겹치는 영역에 더 많은 가상 샘플을 생성하는 방식입니다. 


-----------------------------------------------------------------------------------------------------------------
내일 저녁 8시

1. 각자 맡은 방법으로 오버샘플링 후 최종 모델학습하고 평가지표 가져오기

필요하시다면, 사이트를 참고해주세요

https://velog.io/@choonsik_mom/oversampling%EA%B3%BC%EB%8C%80%ED%91%9C%EC%A7%91-%EA%B8%B0%EB%B2%95-%EC%A0%81%EC%9A%A9%ED%95%B4-%EB%B3%B4%EA%B8%B0

2. 최종발표 때 사용할 ppt 기본 구상도 생각해오기

 -> eda 결과 붙일 살 같은 것들... 생각해보기
